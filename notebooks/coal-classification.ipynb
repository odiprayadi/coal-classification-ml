# ==================================================
# üì¶ IMPORT LIBRARY
# ==================================================
import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import load_img, img_to_array
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from sklearn.utils.class_weight import compute_class_weight
import random
from sklearn.metrics.pairwise import cosine_similarity
import glob
from collections import Counter
import shutil

# ==================================================
# üìÅ STEP 1: Load Dataset from Local Directory
# ==================================================
raw_path = '/content/coal_data_set/Coal Classification'
print("Dataset berada di:", raw_path)
print("Isi dari direktori awal:")
print(os.listdir(raw_path))

# Hapus folder .ipynb_checkpoints jika ada
checkpoints_dir = os.path.join(raw_path, '.ipynb_checkpoints')
if os.path.exists(checkpoints_dir):
    shutil.rmtree(checkpoints_dir)
    print("Folder '.ipynb_checkpoints' telah dihapus.")

# Tentukan folder baru untuk dataset yang terorganisir
organized_path = '/content/coal_data_set/Coal Classification Organized'
os.makedirs(organized_path, exist_ok=True)

# Pisahkan gambar ke dalam folder per kelas berdasarkan nama file
for folder_name in os.listdir(raw_path):
    folder_path = os.path.join(raw_path, folder_name)

    # Lewati jika bukan folder
    if not os.path.isdir(folder_path):
        continue

    # Tentukan label berdasarkan folder (Anthracite, Bituminous, Lignite, Peat)
    if folder_name.lower() == "anthracite":
        label = "Anthracite"
    elif folder_name.lower() == "bituminous":
        label = "Bituminous"
    elif folder_name.lower() == "lignite":
        label = "Lignite"
    elif folder_name.lower() == "peat":
        label = "Peat"
    else:
        continue

    # Buat folder kelas dalam organized_path jika belum ada
    class_dir = os.path.join(organized_path, label)
    os.makedirs(class_dir, exist_ok=True)

    # Pindahkan file dari folder ke folder terorganisir
    for fname in os.listdir(folder_path):
        file_path = os.path.join(folder_path, fname)
        if os.path.isfile(file_path):
            shutil.copy(file_path, os.path.join(class_dir, fname))

print("\nOrganisasi selesai. Folder hasil klasifikasi per jenis batubara:")
print(os.listdir(organized_path))

# ==================================================
# üñºÔ∏è STEP 2: Preprocess & Load Dataset
# ==================================================
img_size = (224, 224)
batch_size = 32

datagen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    validation_split=0.2,
    rotation_range=10,
    zoom_range=0.1,
    shear_range=0.1,
    horizontal_flip=True
)

train = datagen.flow_from_directory(
    organized_path,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='sparse',
    subset='training',
    shuffle=True
)

val = datagen.flow_from_directory(
    organized_path,
    target_size=img_size,
    batch_size=batch_size,
    class_mode='sparse',
    subset='validation'
)

# Optional: check class distribution
y_train = train.classes
print("Train class distribution:", Counter(y_train))

# ==================================================
# üß† STEP 3: Load Pre-trained MobileNetV2
# ==================================================
base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')
base_model.trainable = False  # Freeze base layers

# ==================================================
# üß± STEP 4: Custom Classifier Head
# ==================================================
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(train.num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ==================================================
# ‚öñÔ∏è STEP 5: Compute Class Weights
# ==================================================
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

# ==================================================
# üîÅ STEP 6: Train Model
# ==================================================
history = model.fit(train, validation_data=val, epochs=10, class_weight=class_weights_dict)

# ==================================================
# üìä STEP 7: Visualize Training Results
# ==================================================
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Loss')
plt.legend()
plt.show()

# ==================================================
# üß™ STEP 8: Evaluate Model
# ==================================================
val_preds = model.predict(val)
y_pred = np.argmax(val_preds, axis=1)
y_true = val.classes

# Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d',
            xticklabels=val.class_indices.keys(),
            yticklabels=val.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print(classification_report(y_true, y_pred, target_names=val.class_indices.keys()))

# Akurasi Validasi
val_loss, val_acc = model.evaluate(val)
print(f"Validation Accuracy: {val_acc * 100:.2f}%")

# ==================================================
# üíæ STEP 9: Save Model
# ==================================================
model.save('coal_transfer_model.h5')

# ==================================================
# üß† STEP 10: Inference - Predict Custom Image
# ==================================================
img_path = '/content/testt.jpeg'  # Change to your test image
img = load_img(img_path, target_size=img_size)
img_array = img_to_array(img)
img_array = preprocess_input(img_array)
img_array = np.expand_dims(img_array, axis=0)

# Label mapping based on class_indices
class_indices = val.class_indices
id_to_label = {v: k for k, v in class_indices.items()}

pred = model.predict(img_array)
pred_class = np.argmax(pred)
confidence = np.max(pred) * 100

# Show result
plt.imshow(load_img(img_path))
plt.axis('off')
plt.title(f"Prediction: {id_to_label[pred_class]} ({confidence:.2f}%)")
plt.show()

print(f"Prediction result: {id_to_label[pred_class]} ({confidence:.2f}%)")

# ==================================================
# üîç BONUS: Find Similar Images
# ==================================================
def find_similar_images(input_img_path, dataset_path, base_model, img_size=(224, 224), top_k=3):
    input_img = load_img(input_img_path, target_size=img_size)
    input_array = img_to_array(input_img)
    input_array = preprocess_input(input_array)
    input_array = np.expand_dims(input_array, axis=0)
    input_feature = base_model.predict(input_array)

    similarities = []
    image_paths = glob.glob(os.path.join(dataset_path, "*", "*.jpg"))

    for img_file in image_paths:
        img = load_img(img_file, target_size=img_size)
        img_array = img_to_array(img)
        img_array = preprocess_input(img_array)
        img_array = np.expand_dims(img_array, axis=0)
        feat = base_model.predict(img_array)
        sim = cosine_similarity(input_feature.reshape(1, -1), feat.reshape(1, -1))[0][0]
        similarities.append((img_file, sim))

    similarities.sort(key=lambda x: x[1], reverse=True)

    for i in range(top_k):
        print(f"Top-{i+1} match: {similarities[i][0]}, similarity: {similarities[i][1]:.4f}")
        plt.imshow(load_img(similarities[i][0]))
        plt.title(f"Match {i+1}")
        plt.axis('off')
        plt.show()

# ==================================================
# üì∑ BONUS: Show Example Images from Predicted Class
# ==================================================
def show_examples_from_predicted_class(predicted_label, dataset_path, max_samples=3):
    folder_path = os.path.join(dataset_path, predicted_label)
    if not os.path.exists(folder_path):
        print(f"Folder for class {predicted_label} not found in {dataset_path}")
        return

    sample_images = glob.glob(os.path.join(folder_path, "*.jpg"))
    if len(sample_images) == 0:
        print(f"No images found in the folder {folder_path}")
        return

    sample_images = random.sample(sample_images, min(len(sample_images), max_samples))
    for img_file in sample_images:
        img = load_img(img_file)
        plt.imshow(img)
        plt.title(f"Example from Class {predicted_label}")
        plt.axis('off')
        plt.show()

# ‚úÖ Show example images from prediction
show_examples_from_predicted_class(id_to_label[pred_class], raw_path)
